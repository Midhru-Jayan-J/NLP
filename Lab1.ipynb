{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ae110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Input\n",
    "sentence= input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee827ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing in NLTK involves cleaning and preparing raw text data for analysis. It helps in handling symbols like \"@\", \"#\", or \"&\", numbers like \"123\" or \"99.9\", and abbreviations like \"U.S.A.\" or \"etc.\" Common steps include tokenization, removing unwanted characters, and normalizing text. This ensures better performance in tasks like sentiment analysis, classification, or language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ddf8a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Preprocessing in NLTK involves cleaning and preparing raw text data for analysis. It helps in handling symbols like \"@\", \"#\", or \"&\", numbers like \"123\" or \"99.9\", and abbreviations like \"U.S.A.\" or \"etc.\" Common steps include tokenization, removing unwanted characters, and normalizing text. This ensures better performance in tasks like sentiment analysis, classification, or language modeling.\n"
     ]
    }
   ],
   "source": [
    "print(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5b9d586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Preprocessing in NLTK involves cleaning and preparing raw text data for analysis. It helps in handling symbols like \"@\", \"#\", or \"&\", numbers like \"123\" or \"99.9\", and abbreviations like \"U.S.A.\" or \"etc.\" Common steps include tokenization, removing unwanted characters, and normalizing text. This ensures better performance in tasks like sentiment analysis, classification, or language modeling.\n"
     ]
    }
   ],
   "source": [
    "# 2. Basic Preprocess - removing extra spaces\n",
    "cleaned_text=\" \".join(sentence.split())\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d6cb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#preprocessing in nltk involves cleaning and preparing raw text data for analysis. it helps in handling symbols like \"@\", \"#\", or \"&\", numbers like \"123\" or \"99.9\", and abbreviations like \"u.s.a.\" or \"etc.\" common steps include tokenization, removing unwanted characters, and normalizing text. this ensures better performance in tasks like sentiment analysis, classification, or language modeling.\n"
     ]
    }
   ],
   "source": [
    "# 3.Lower Case\n",
    "lower_text=cleaned_text.lower()\n",
    "print(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a338cbc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'preprocessing', 'in', 'nltk', 'involves', 'cleaning', 'and', 'preparing', 'raw', 'text', 'data', 'for', 'analysis', '.', 'it', 'helps', 'in', 'handling', 'symbols', 'like', '``', '@', \"''\", ',', '``', '#', \"''\", ',', 'or', '``', '&', \"''\", ',', 'numbers', 'like', '``', '123', \"''\", 'or', '``', '99.9', \"''\", ',', 'and', 'abbreviations', 'like', '``', 'u.s.a.', \"''\", 'or', '``', 'etc', '.', \"''\", 'common', 'steps', 'include', 'tokenization', ',', 'removing', 'unwanted', 'characters', ',', 'and', 'normalizing', 'text', '.', 'this', 'ensures', 'better', 'performance', 'in', 'tasks', 'like', 'sentiment', 'analysis', ',', 'classification', ',', 'or', 'language', 'modeling', '.']\n"
     ]
    }
   ],
   "source": [
    "# 4. Tokenization \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize #word - for each word , sent-for each sentence\n",
    "tokens=word_tokenize(lower_text)\n",
    "print(tokens)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86c8b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessing', 'in', 'nltk', 'involves', 'cleaning', 'and', 'preparing', 'raw', 'text', 'data', 'for', 'analysis', 'it', 'helps', 'in', 'handling', 'symbols', 'like', '``', \"''\", '``', \"''\", 'or', '``', \"''\", 'numbers', 'like', '``', '123', \"''\", 'or', '``', '99.9', \"''\", 'and', 'abbreviations', 'like', '``', 'u.s.a.', \"''\", 'or', '``', 'etc', \"''\", 'common', 'steps', 'include', 'tokenization', 'removing', 'unwanted', 'characters', 'and', 'normalizing', 'text', 'this', 'ensures', 'better', 'performance', 'in', 'tasks', 'like', 'sentiment', 'analysis', 'classification', 'or', 'language', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "# 5.Remove Puntuation and symbols \n",
    "import string\n",
    "tokens_without_pun=[token for token in tokens if token not in string.punctuation]\n",
    "print(tokens_without_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f73666c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['preprocessing', 'nltk', 'involves', 'cleaning', 'preparing', 'raw', 'text', 'data', 'analysis', 'helps', 'handling', 'symbols', 'like', '``', \"''\", '``', \"''\", '``', \"''\", 'numbers', 'like', '``', '123', \"''\", '``', '99.9', \"''\", 'abbreviations', 'like', '``', 'u.s.a.', \"''\", '``', 'etc', \"''\", 'common', 'steps', 'include', 'tokenization', 'removing', 'unwanted', 'characters', 'normalizing', 'text', 'ensures', 'better', 'performance', 'tasks', 'like', 'sentiment', 'analysis', 'classification', 'language', 'modeling']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 6. Remove Stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "no_stop = [token for token in tokens_without_pun if token not in stop_words]\n",
    "print(no_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9671afd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing -> preprocess\n",
      "involves -> involv\n",
      "cleaning -> clean\n",
      "preparing -> prepar\n",
      "analysis -> analysi\n",
      "helps -> help\n",
      "handling -> handl\n",
      "symbols -> symbol\n",
      "numbers -> number\n",
      "abbreviations -> abbrevi\n",
      "steps -> step\n",
      "include -> includ\n",
      "tokenization -> token\n",
      "removing -> remov\n",
      "unwanted -> unwant\n",
      "characters -> charact\n",
      "normalizing -> normal\n",
      "ensures -> ensur\n",
      "performance -> perform\n",
      "tasks -> task\n",
      "analysis -> analysi\n",
      "classification -> classif\n",
      "language -> languag\n",
      "modeling -> model\n",
      "['preprocess', 'nltk', 'involv', 'clean', 'prepar', 'raw', 'text', 'data', 'analysi', 'help', 'handl', 'symbol', 'like', '``', \"''\", '``', \"''\", '``', \"''\", 'number', 'like', '``', '123', \"''\", '``', '99.9', \"''\", 'abbrevi', 'like', '``', 'u.s.a.', \"''\", '``', 'etc', \"''\", 'common', 'step', 'includ', 'token', 'remov', 'unwant', 'charact', 'normal', 'text', 'ensur', 'better', 'perform', 'task', 'like', 'sentiment', 'analysi', 'classif', 'languag', 'model']\n"
     ]
    }
   ],
   "source": [
    "# 7.Stemming\n",
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_token = [stemmer.stem(token) for token in no_stop]\n",
    "for origi,stem in zip(no_stop,stemmed_token):\n",
    "    if origi != stem:\n",
    "        print(f\"{origi} -> {stem}\")\n",
    "print(stemmed_token)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08d9c798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helps -> help\n",
      "symbols -> symbol\n",
      "numbers -> number\n",
      "abbreviations -> abbreviation\n",
      "steps -> step\n",
      "characters -> character\n",
      "tasks -> task\n",
      "['preprocessing', 'nltk', 'involves', 'cleaning', 'preparing', 'raw', 'text', 'data', 'analysis', 'help', 'handling', 'symbol', 'like', '``', \"''\", '``', \"''\", '``', \"''\", 'number', 'like', '``', '123', \"''\", '``', '99.9', \"''\", 'abbreviation', 'like', '``', 'u.s.a.', \"''\", '``', 'etc', \"''\", 'common', 'step', 'include', 'tokenization', 'removing', 'unwanted', 'character', 'normalizing', 'text', 'ensures', 'better', 'performance', 'task', 'like', 'sentiment', 'analysis', 'classification', 'language', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "# 8. Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "lem_token = [lem.lemmatize(token) for token in no_stop]\n",
    "for origi, stem in zip(no_stop, lem_token):\n",
    "    if origi != stem:\n",
    "        print(f\"{origi} -> {stem}\")\n",
    "print(lem_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da3a60d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final stemmed (no numbers/special chars): ['preprocess', 'nltk', 'involv', 'clean', 'prepar', 'raw', 'text', 'data', 'analysi', 'help', 'handl', 'symbol', 'like', 'number', 'like', 'abbrevi', 'like', 'etc', 'common', 'step', 'includ', 'token', 'remov', 'unwant', 'charact', 'normal', 'text', 'ensur', 'better', 'perform', 'task', 'like', 'sentiment', 'analysi', 'classif', 'languag', 'model']\n",
      "Final lemmatized (no numbers/special chars): ['preprocessing', 'nltk', 'involves', 'cleaning', 'preparing', 'raw', 'text', 'data', 'analysis', 'help', 'handling', 'symbol', 'like', 'number', 'like', 'abbreviation', 'like', 'etc', 'common', 'step', 'include', 'tokenization', 'removing', 'unwanted', 'character', 'normalizing', 'text', 'ensures', 'better', 'performance', 'task', 'like', 'sentiment', 'analysis', 'classification', 'language', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "# 8. Remove numbers and special characters\n",
    "import re\n",
    "final_stemmed = []\n",
    "final_lemmatized = []\n",
    "for token in stemmed_token:\n",
    "    if not re.search(r'\\d', token) and token.isalpha() and len(token) > 1:\n",
    "        final_stemmed.append(token)\n",
    "for token in lem_token:\n",
    "    if not re.search(r'\\d', token) and token.isalpha() and len(token) > 1:\n",
    "        final_lemmatized.append(token)\n",
    "print(f\"Final stemmed (no numbers/special chars): {final_stemmed}\")\n",
    "print(f\"Final lemmatized (no numbers/special chars): {final_lemmatized}\")\n",
    "results = {}\n",
    "results['final_stemmed'] = final_stemmed\n",
    "results['final_lemmatized'] = final_lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8c3629c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 10 - POS Tagging:\n",
      "POS Tags: [('preprocessing', 'VBG'), ('nltk', 'JJ'), ('involves', 'NNS'), ('cleaning', 'VBG'), ('preparing', 'VBG'), ('raw', 'JJ'), ('text', 'NN'), ('data', 'NNS'), ('analysis', 'NN'), ('helps', 'VBZ'), ('handling', 'VBG'), ('symbols', 'NNS'), ('like', 'IN'), ('``', '``'), (\"''\", \"''\"), ('``', '``'), (\"''\", \"''\"), ('``', '``'), (\"''\", \"''\"), ('numbers', 'NNS'), ('like', 'IN'), ('``', '``'), ('123', 'CD'), (\"''\", \"''\"), ('``', '``'), ('99.9', 'CD'), (\"''\", \"''\"), ('abbreviations', 'NNS'), ('like', 'IN'), ('``', '``'), ('u.s.a.', 'JJ'), (\"''\", \"''\"), ('``', '``'), ('etc', 'FW'), (\"''\", \"''\"), ('common', 'JJ'), ('steps', 'NNS'), ('include', 'VBP'), ('tokenization', 'NN'), ('removing', 'VBG'), ('unwanted', 'JJ'), ('characters', 'NNS'), ('normalizing', 'VBG'), ('text', 'NN'), ('ensures', 'NNS'), ('better', 'RBR'), ('performance', 'NN'), ('tasks', 'NNS'), ('like', 'IN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('classification', 'NN'), ('language', 'NN'), ('modeling', 'NN')]\n",
      "POS Tags (final): [('preprocessing', 'VBG'), ('nltk', 'JJ'), ('involves', 'NNS'), ('cleaning', 'VBG'), ('preparing', 'VBG'), ('raw', 'JJ'), ('text', 'NN'), ('data', 'NNS'), ('analysis', 'NN'), ('help', 'NN'), ('handling', 'VBG'), ('symbol', 'NN'), ('like', 'IN'), ('number', 'NN'), ('like', 'IN'), ('abbreviation', 'NN'), ('like', 'IN'), ('etc', 'FW'), ('common', 'JJ'), ('step', 'NN'), ('include', 'VBP'), ('tokenization', 'NN'), ('removing', 'VBG'), ('unwanted', 'JJ'), ('character', 'NN'), ('normalizing', 'VBG'), ('text', 'JJ'), ('ensures', 'NNS'), ('better', 'RBR'), ('performance', 'NN'), ('task', 'NN'), ('like', 'IN'), ('sentiment', 'NN'), ('analysis', 'NN'), ('classification', 'NN'), ('language', 'NN'), ('modeling', 'NN')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'final_stemmed': ['preprocess',\n",
       "  'nltk',\n",
       "  'involv',\n",
       "  'clean',\n",
       "  'prepar',\n",
       "  'raw',\n",
       "  'text',\n",
       "  'data',\n",
       "  'analysi',\n",
       "  'help',\n",
       "  'handl',\n",
       "  'symbol',\n",
       "  'like',\n",
       "  'number',\n",
       "  'like',\n",
       "  'abbrevi',\n",
       "  'like',\n",
       "  'etc',\n",
       "  'common',\n",
       "  'step',\n",
       "  'includ',\n",
       "  'token',\n",
       "  'remov',\n",
       "  'unwant',\n",
       "  'charact',\n",
       "  'normal',\n",
       "  'text',\n",
       "  'ensur',\n",
       "  'better',\n",
       "  'perform',\n",
       "  'task',\n",
       "  'like',\n",
       "  'sentiment',\n",
       "  'analysi',\n",
       "  'classif',\n",
       "  'languag',\n",
       "  'model'],\n",
       " 'final_lemmatized': ['preprocessing',\n",
       "  'nltk',\n",
       "  'involves',\n",
       "  'cleaning',\n",
       "  'preparing',\n",
       "  'raw',\n",
       "  'text',\n",
       "  'data',\n",
       "  'analysis',\n",
       "  'help',\n",
       "  'handling',\n",
       "  'symbol',\n",
       "  'like',\n",
       "  'number',\n",
       "  'like',\n",
       "  'abbreviation',\n",
       "  'like',\n",
       "  'etc',\n",
       "  'common',\n",
       "  'step',\n",
       "  'include',\n",
       "  'tokenization',\n",
       "  'removing',\n",
       "  'unwanted',\n",
       "  'character',\n",
       "  'normalizing',\n",
       "  'text',\n",
       "  'ensures',\n",
       "  'better',\n",
       "  'performance',\n",
       "  'task',\n",
       "  'like',\n",
       "  'sentiment',\n",
       "  'analysis',\n",
       "  'classification',\n",
       "  'language',\n",
       "  'modeling'],\n",
       " 'pos_tags': [('preprocessing', 'VBG'),\n",
       "  ('nltk', 'JJ'),\n",
       "  ('involves', 'NNS'),\n",
       "  ('cleaning', 'VBG'),\n",
       "  ('preparing', 'VBG'),\n",
       "  ('raw', 'JJ'),\n",
       "  ('text', 'NN'),\n",
       "  ('data', 'NNS'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('helps', 'VBZ'),\n",
       "  ('handling', 'VBG'),\n",
       "  ('symbols', 'NNS'),\n",
       "  ('like', 'IN'),\n",
       "  ('``', '``'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('``', '``'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('``', '``'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('numbers', 'NNS'),\n",
       "  ('like', 'IN'),\n",
       "  ('``', '``'),\n",
       "  ('123', 'CD'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('``', '``'),\n",
       "  ('99.9', 'CD'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('abbreviations', 'NNS'),\n",
       "  ('like', 'IN'),\n",
       "  ('``', '``'),\n",
       "  ('u.s.a.', 'JJ'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('``', '``'),\n",
       "  ('etc', 'FW'),\n",
       "  (\"''\", \"''\"),\n",
       "  ('common', 'JJ'),\n",
       "  ('steps', 'NNS'),\n",
       "  ('include', 'VBP'),\n",
       "  ('tokenization', 'NN'),\n",
       "  ('removing', 'VBG'),\n",
       "  ('unwanted', 'JJ'),\n",
       "  ('characters', 'NNS'),\n",
       "  ('normalizing', 'VBG'),\n",
       "  ('text', 'NN'),\n",
       "  ('ensures', 'NNS'),\n",
       "  ('better', 'RBR'),\n",
       "  ('performance', 'NN'),\n",
       "  ('tasks', 'NNS'),\n",
       "  ('like', 'IN'),\n",
       "  ('sentiment', 'NN'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('classification', 'NN'),\n",
       "  ('language', 'NN'),\n",
       "  ('modeling', 'NN')],\n",
       " 'pos_tags_final': [('preprocessing', 'VBG'),\n",
       "  ('nltk', 'JJ'),\n",
       "  ('involves', 'NNS'),\n",
       "  ('cleaning', 'VBG'),\n",
       "  ('preparing', 'VBG'),\n",
       "  ('raw', 'JJ'),\n",
       "  ('text', 'NN'),\n",
       "  ('data', 'NNS'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('help', 'NN'),\n",
       "  ('handling', 'VBG'),\n",
       "  ('symbol', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('number', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('abbreviation', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('etc', 'FW'),\n",
       "  ('common', 'JJ'),\n",
       "  ('step', 'NN'),\n",
       "  ('include', 'VBP'),\n",
       "  ('tokenization', 'NN'),\n",
       "  ('removing', 'VBG'),\n",
       "  ('unwanted', 'JJ'),\n",
       "  ('character', 'NN'),\n",
       "  ('normalizing', 'VBG'),\n",
       "  ('text', 'JJ'),\n",
       "  ('ensures', 'NNS'),\n",
       "  ('better', 'RBR'),\n",
       "  ('performance', 'NN'),\n",
       "  ('task', 'NN'),\n",
       "  ('like', 'IN'),\n",
       "  ('sentiment', 'NN'),\n",
       "  ('analysis', 'NN'),\n",
       "  ('classification', 'NN'),\n",
       "  ('language', 'NN'),\n",
       "  ('modeling', 'NN')]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS Tagging\n",
    "from nltk import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# POS Tagging\n",
    "print(\"\\nStep 10 - POS Tagging:\")\n",
    "pos_tags = pos_tag(no_stop)\n",
    "print(f\"POS Tags: {pos_tags}\")\n",
    "results['pos_tags'] = pos_tags\n",
    "\n",
    "# POS tagging on final processed tokens\n",
    "pos_tags_final = pos_tag(final_lemmatized)\n",
    "print(f\"POS Tags (final): {pos_tags_final}\")\n",
    "results['pos_tags_final'] = pos_tags_final\n",
    "\n",
    "results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
